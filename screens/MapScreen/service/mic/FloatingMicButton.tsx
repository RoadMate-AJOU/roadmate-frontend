import React, { useRef, useState, useEffect } from 'react';
import { Animated, TouchableOpacity, View } from 'react-native';
import { Ionicons } from '@expo/vector-icons';
import * as Speech from 'expo-speech';
import {
  ExpoSpeechRecognitionModule,
  useSpeechRecognitionEvent,
} from 'expo-speech-recognition';
import { getVoiceOwner, setVoiceOwner, clearVoiceOwner } from '@/hooks/VoiceOwner';
import { gptService } from '@/services/api';
import { router } from 'expo-router';
import { styles } from '../../style/FloatingMicButton.styles'; // âœ… ìŠ¤íƒ€ì¼ ë¶„ë¦¬
import { useRoute } from '../../model/RouteContext';
import { fetchBusArrivalTime } from '../transportTime/fetchBusArrivalTime';
import { fetchSubwayArrivalTime } from '../transportTime/fetchSubwayArrivalTime';
import { useLocation } from '@/contexts/LocationContext';


export default function FloatingMicButton() {
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const recognizedTextRef = useRef('');
  const { routeData } = useRoute();
  const pulseAnim = useRef(new Animated.Value(1)).current;
  const glowAnim = useRef(new Animated.Value(0)).current;

  useEffect(() => {
    if (isSpeaking) {
      Animated.loop(
        Animated.sequence([
          Animated.timing(pulseAnim, { toValue: 1.1, duration: 800, useNativeDriver: true }),
          Animated.timing(pulseAnim, { toValue: 1, duration: 800, useNativeDriver: true }),
        ])
      ).start();

      Animated.loop(
        Animated.sequence([
          Animated.timing(glowAnim, { toValue: 1, duration: 1000, useNativeDriver: false }),
          Animated.timing(glowAnim, { toValue: 0.3, duration: 1000, useNativeDriver: false }),
        ])
      ).start();
    } else {
      pulseAnim.setValue(1);
      glowAnim.setValue(0);
    }
  }, [isSpeaking]);

  const handleMicPress = async () => {
    if (isListening) {
      try {
        await ExpoSpeechRecognitionModule.stop();
      } catch (err) {
        console.error('âŒ ìŒì„± ì¸ì‹ ì¢…ë£Œ ì˜¤ë¥˜:', err);
      }
    } else {
      try {
        setVoiceOwner('mic');
        await ExpoSpeechRecognitionModule.start({
          lang: 'ko-KR',
          continuous: false,
          interimResults: true,
        });
        setIsListening(true);
        setIsSpeaking(true);
      } catch (err) {
        console.error('âŒ ìŒì„± ì¸ì‹ ì‹œì‘ ì˜¤ë¥˜:', err);
      }
    }
  };

  useSpeechRecognitionEvent('result', (event) => {
    const finalText = event.results?.[0]?.transcript;
    if (finalText) {
      recognizedTextRef.current = finalText;
      console.log('âœ… ì¸ì‹ëœ í…ìŠ¤íŠ¸:', finalText);
    }
  });

  useSpeechRecognitionEvent('end', () => {
    if (getVoiceOwner() !== 'mic') return;
    setIsListening(false);
    clearVoiceOwner();

    const finalText = recognizedTextRef.current;
    if (finalText) {
      Speech.speak(finalText, {
        language: 'ko-KR',
        onDone: () => {
          console.log('âœ… TTS ì™„ë£Œ, GPT ì „ì†¡');
          sendToBackend(finalText);
        },
        onError: (err) => {
          console.error('âŒ TTS ì˜¤ë¥˜:', err);
          setIsSpeaking(false);
        },
      });
    } else {
      console.log('âš ï¸ ì¸ì‹ëœ í…ìŠ¤íŠ¸ ì—†ìŒ');
      setIsSpeaking(false);
    }
  });

  useSpeechRecognitionEvent('error', (event) => {
    console.error('âŒ ìŒì„± ì¸ì‹ ì—ëŸ¬:', event.error);
    setIsListening(false);
    setIsSpeaking(false);
  });

  const sendToBackend = async (text: string) => {
    try {
      const json = await gptService.askQuestion(text);
      const { status, intent, responseMessage, data } = json;

//       const { currentLegIndex } = useLocation(); // ğŸ”¥ í˜„ì¬ í•˜ì´ë¼ì´íŠ¸ëœ ì¹´ë“œ index
// const validLegIndex = currentLegIndex < 0 ? 0 : currentLegIndex;
// const currentGuide = routeData?.guides?.[validLegIndex];

      if (!responseMessage) {
        setIsSpeaking(false);
        return;
      }

      Speech.speak(responseMessage, {
        language: 'ko-KR',
        onDone: async () => {
          if (status === 'API_REQUIRED') {
            if (intent === 'real_time_bus_arrival' && routeData && data?.bus_number) {
  const targetBusNumber = data.bus_number;

  const busGuide = routeData.guides.find(
    (g) => g.transportType === 'BUS' && g.busNumber === targetBusNumber
  );

  if (busGuide?.startLocation?.name) {
    const stopName = busGuide.startLocation.name;
    const arrival = await fetchBusArrivalTime(stopName, targetBusNumber);

    let info = 'ë„ì°© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
    if (arrival === 'ìš´í–‰ì¢…ë£Œ') info = 'ìš´í–‰ì´ ì¢…ë£Œëœ ë²„ìŠ¤ì…ë‹ˆë‹¤.';
    else if (arrival === 'ì¶œë°œëŒ€ê¸° ì¤‘') info = 'ì¶œë°œ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.';
    else if (typeof arrival === 'number') info = arrival === 0 ? 'ê³§ ë„ì°©í•©ë‹ˆë‹¤.' : `${arrival}ë¶„ í›„ ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤.`;

    Speech.speak(`${targetBusNumber}ë²ˆ ë²„ìŠ¤, ${stopName} ì •ë¥˜ì¥ ê¸°ì¤€ ${info}`, {
      language: 'ko-KR',
      onDone: () => setIsSpeaking(false),
    });
    return;
  } else {
    Speech.speak(`${targetBusNumber}ë²ˆ ë²„ìŠ¤ì˜ ì •ë¥˜ì¥ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`, {
      language: 'ko-KR',
      onDone: () => setIsSpeaking(false),
    });
    return;
  }
}


            else if (intent === 'real_time_subway_arrival' && routeData && data?.subway_line) {
  const targetLine = data.subway_line;

  const subwayGuide = routeData.guides.find(
    (g) => g.transportType === 'SUBWAY' && g.routeName === targetLine
  );

  if (subwayGuide?.startLocation?.name && subwayGuide?.routeName) {
    const stationName = subwayGuide.startLocation.name;
    const lineName = subwayGuide.routeName;

    const arrivalMin = await fetchSubwayArrivalTime(stationName, lineName);

    const info = typeof arrivalMin === 'number'
      ? (arrivalMin === 0 ? 'ê³§ ë„ì°©í•©ë‹ˆë‹¤.' : `${arrivalMin}ë¶„ í›„ ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤.`)
      : 'ë„ì°© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';

    Speech.speak(`${lineName}, ${stationName}ì—­ ê¸°ì¤€ ${info}`, {
      language: 'ko-KR',
      onDone: () => setIsSpeaking(false),
    });
    return;
  } else {
    Speech.speak(`${targetLine} ì§€í•˜ì²  ë…¸ì„ ì˜ ì—­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`, {
      language: 'ko-KR',
      onDone: () => setIsSpeaking(false),
    });
    return;
  }
}
// else if (intent === 'current_location' && currentGuide) {
//             const guidanceText = currentGuide.guidance || 'ì•ˆë‚´ ë¬¸êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤';
//             Speech.speak(guidanceText, {
//               language: 'ko-KR',
//               onDone: () => setIsSpeaking(false),
//             });
//             return;
//           }

            else if (['extract_route', 'research_route'].includes(intent)) {
              Speech.speak('í™ˆìœ¼ë¡œ ì´ë™í• ê²Œìš”. ëª©ì ì§€ë¥¼ ë‹¤ì‹œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”.', {
                language: 'ko-KR',
                onDone: () => {
                  setIsSpeaking(false);
                  router.replace('/(tabs)');
                },
              });
              return;
            }
          }

          setIsSpeaking(false);
        },
        onError: () => setIsSpeaking(false),
      });
    } catch (err) {
      console.error('âŒ GPT í†µì‹  ì˜¤ë¥˜:', err);
      setIsSpeaking(false);
    }
  };

  const glowOpacity = glowAnim.interpolate({
    inputRange: [0, 1],
    outputRange: [0, 0.6],
  });

  const glowRadius = glowAnim.interpolate({
    inputRange: [0, 1],
    outputRange: [0, 20],
  });

  return (
  <View style={styles.container}>
    {isSpeaking && (
      <Animated.View
        style={[
          styles.glowEffect,
          {
            opacity: glowOpacity,
            shadowRadius: glowRadius,
          },
        ]}
      />
    )}
    <Animated.View
      style={[
        styles.micButton,
        {
          transform: [{ scale: pulseAnim }],
          backgroundColor: isSpeaking ? '#FF3B30' : '#FF5900',
        },
      ]}
    >
      <TouchableOpacity
        onPress={handleMicPress}
        style={styles.touchArea}
        activeOpacity={0.8}
      >
        <Ionicons name={isSpeaking ? 'stop' : 'mic'} size={28} color="white" />
      </TouchableOpacity>
    </Animated.View>
  </View>
);

}
